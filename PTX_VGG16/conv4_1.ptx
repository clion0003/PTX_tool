.reg .u32 %r<30>;
.reg .u64 %rd<30>;
.reg .f32 %f<3500>;
.reg .pred %p<30>;

cvt.s32.u16 %r1, %tid.x;
cvt.s64.s32 %rd1, %r1;
mul.wide.s32 %rd2, %r1, 4;
ld.param.u64 %rd3, [_Z7conv4_1IfEvPT_PKS0_S3_S3__param_1];
add.u64 %rd4, %rd3, %rd2;
ld.param.u64 %rd5, [_Z7conv4_1IfEvPT_PKS0_S3_S3__param_2];
add.u64 %rd6, %rd5, %rd2;
ld.param.u64 %rd7, [_Z7conv4_1IfEvPT_PKS0_S3_S3__param_0];
add.u64 %rd8, %rd7, %rd2;
ld.param.u64 %rd9, [_Z7conv4_1IfEvPT_PKS0_S3_S3__param_3];

mov.s32 %r21, 128;
mov.s32 %r19, 128;
mov.s32 %r22, 30;
mov.u64 %rd12, %rd8;
mov.u64 %rd11, %rd6;
mov.u64 %rd10, %rd4;
mov.s32 %r23, 0;
$Lt_0:
mov.u64 %rd14, %rd9;
mov.u64 %rd16, %rd12;
mov.s32 %r20, 0;
$Lt_0_0:
ld.global.f32 %f250, [%rd14+0];
ld.global.f32 %f251, [%rd14+4];
ld.global.f32 %f252, [%rd14+8];
ld.global.f32 %f253, [%rd14+12];
mov.f32 %f200, %f250;
mov.f32 %f201, %f250;
mov.f32 %f202, %f251;
mov.f32 %f203, %f251;
mov.f32 %f204, %f252;
mov.f32 %f205, %f252;
mov.f32 %f206, %f253;
mov.f32 %f207, %f253;
ldc.global.f32 %f4,[%rd11+0], 0, 36;
ldc.global.f32 %f10, [%rd10+0], 0, 32;
ldc.global.f32 %f12, [%rd10+128], 3, 32;
ldc.global.f32 %f14, [%rd10+256], 6, 30;
ldc.global.f32 %f15, [%rd10+376], 0, 2;
add.u64 %rd15, %rd10, 4096;
add.u64 %rd13, %rd11, 18432;
mov.s32 %r18, 0;
$Lt_0_0_0:
ldc.global.f32 %f5,[%rd13], 0, 36;
ldc.global.f32 %f16, [%rd15+0], 0, 32;
ldc.global.f32 %f18, [%rd15+128], 3, 32;
ldc.global.f32 %f20, [%rd15+256], 6, 30;
ldc.global.f32 %f21, [%rd15+376], 0, 2;
fillw.f32 %f4;
shiftfull.f32 %f200, 20, %f10, 0, 2, 2;
shiftcol.f32 %f200, 21, %f10, 3, 2, 2;
shiftcol.f32 %f200, 22, %f10, 4, 2, 2;
shiftcol.f32 %f200, 23, %f10, 5, 2, 2;
shiftcol.f32 %f200, 24, %f10, 6, 2, 2;
shiftcol.f32 %f200, 25, %f10, 7, 2, 2;
shiftcol.f32 %f200, 26, %f10, 8, 2, 2;
shiftcol.f32 %f200, 27, %f10, 9, 2, 2;
shiftcol.f32 %f200, 28, %f10, 10, 2, 2;
shiftcol.f32 %f200, 29, %f10, 11, 2, 2;
shiftcol.f32 %f200, 30, %f10, 12, 2, 2;
shiftcol.f32 %f200, 31, %f10, 13, 2, 2;
shiftcol.f32 %f200, 32, %f10, 14, 2, 2;
shiftcol.f32 %f200, 33, %f10, 15, 2, 2;
shiftcol.f32 %f200, 34, %f10, 16, 2, 2;
shiftcol.f32 %f200, 35, %f10, 17, 2, 2;
shiftcol.f32 %f201, 0, %f10, 18, 2, 2;
shiftcol.f32 %f201, 1, %f10, 19, 2, 2;
shiftcol.f32 %f201, 2, %f10, 20, 2, 2;
shiftcol.f32 %f201, 3, %f10, 21, 2, 2;
shiftcol.f32 %f201, 4, %f10, 22, 2, 2;
shiftcol.f32 %f201, 5, %f10, 23, 2, 2;
shiftcol.f32 %f201, 6, %f10, 24, 2, 2;
shiftcol.f32 %f201, 7, %f10, 25, 2, 2;
shiftcol.f32 %f201, 8, %f10, 26, 2, 2;
shiftcol.f32 %f201, 9, %f10, 27, 2, 2;
shiftcol.f32 %f201, 10, %f10, 28, 2, 2;
shiftcol.f32 %f201, 11, %f10, 29, 2, 2;
shiftcol.f32 %f201, 12, %f10, 30, 2, 2;
shiftcol.f32 %f201, 13, %f10, 31, 2, 2;
add.u64 %rd15, %rd15, 4096;
add.u64 %rd13, %rd13, 18432;
ldc.global.f32 %f4,[%rd13], 0, 36;
ldc.global.f32 %f10, [%rd15+0], 0, 32;
ldc.global.f32 %f12, [%rd15+128], 3, 32;
ldc.global.f32 %f14, [%rd15+256], 6, 30;
ldc.global.f32 %f15, [%rd15+376], 0, 2;
fillw.f32 %f5;
shiftfull.f32 %f200, 20, %f16, 0, 2, 2;
shiftcol.f32 %f200, 21, %f16, 3, 2, 2;
shiftcol.f32 %f200, 22, %f16, 4, 2, 2;
shiftcol.f32 %f200, 23, %f16, 5, 2, 2;
shiftcol.f32 %f200, 24, %f16, 6, 2, 2;
shiftcol.f32 %f200, 25, %f16, 7, 2, 2;
shiftcol.f32 %f200, 26, %f16, 8, 2, 2;
shiftcol.f32 %f200, 27, %f16, 9, 2, 2;
shiftcol.f32 %f200, 28, %f16, 10, 2, 2;
shiftcol.f32 %f200, 29, %f16, 11, 2, 2;
shiftcol.f32 %f200, 30, %f16, 12, 2, 2;
shiftcol.f32 %f200, 31, %f16, 13, 2, 2;
shiftcol.f32 %f200, 32, %f16, 14, 2, 2;
shiftcol.f32 %f200, 33, %f16, 15, 2, 2;
shiftcol.f32 %f200, 34, %f16, 16, 2, 2;
shiftcol.f32 %f200, 35, %f16, 17, 2, 2;
shiftcol.f32 %f201, 0, %f16, 18, 2, 2;
shiftcol.f32 %f201, 1, %f16, 19, 2, 2;
shiftcol.f32 %f201, 2, %f16, 20, 2, 2;
shiftcol.f32 %f201, 3, %f16, 21, 2, 2;
shiftcol.f32 %f201, 4, %f16, 22, 2, 2;
shiftcol.f32 %f201, 5, %f16, 23, 2, 2;
shiftcol.f32 %f201, 6, %f16, 24, 2, 2;
shiftcol.f32 %f201, 7, %f16, 25, 2, 2;
shiftcol.f32 %f201, 8, %f16, 26, 2, 2;
shiftcol.f32 %f201, 9, %f16, 27, 2, 2;
shiftcol.f32 %f201, 10, %f16, 28, 2, 2;
shiftcol.f32 %f201, 11, %f16, 29, 2, 2;
shiftcol.f32 %f201, 12, %f16, 30, 2, 2;
shiftcol.f32 %f201, 13, %f16, 31, 2, 2;
add.u64 %rd15, %rd15, 4096;
add.u64 %rd13, %rd13, 18432;
add.s32 %r18, %r18, 1;
setp.ne.s32 %p1, %r18, %r19;
@%p1 bra $Lt_0_0_0;
stc.global.f32 [%rd16+0], %f200, 20, 16;
stc.global.f32 [%rd16+64], %f201, 0, 14;
add.u64 %rd16, %rd16, 120;
add.u64 %rd16, %rd16, 3480;
stc.global.f32 [%rd16+3600], %f202, 21, 15;
stc.global.f32 [%rd16+3660], %f203, 0, 15;
add.u64 %rd16, %rd16, 120;
add.u64 %rd16, %rd16, 3480;
stc.global.f32 [%rd16+7200], %f204, 22, 14;
stc.global.f32 [%rd16+7256], %f205, 0, 16;
add.u64 %rd16, %rd16, 120;
add.u64 %rd16, %rd16, 3480;
stc.global.f32 [%rd16+10800], %f206, 23, 13;
stc.global.f32 [%rd16+10852], %f207, 0, 17;
add.u64 %rd16, %rd16, 120;
add.u64 %rd16, %rd16, 3480;
add.u64 %rd14, %rd14, 16;
add.s32 %r20, %r20, 1;
setp.ne.s32 %p2, %r20, %r21;
@%p2 bra $Lt_0_0;
add.u64 %rd12, %rd12, 120;
add.u64 %rd11, %rd11, 144;
add.u64 %rd10, %rd10, 128;
add.s32 %r23, %r23, 1;
setp.ne.s32 %p3, %r23, %r22;
@%p3 bra $Lt_0;
ret;
